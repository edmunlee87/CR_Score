# CR_Score Test Suite

Comprehensive test coverage for all CR_Score modules.

## Test Structure

```
tests/
â”œâ”€â”€ unit/                # Unit tests for individual modules
â”‚   â”œâ”€â”€ test_models.py          # All 4 model families
â”‚   â”œâ”€â”€ test_evaluation.py      # Evaluation metrics
â”‚   â”œâ”€â”€ test_reporting.py       # Report export
â”‚   â”œâ”€â”€ test_monitoring.py      # Monitoring modules
â”‚   â”œâ”€â”€ test_feature_selection.py
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â””â”€â”€ test_woe_encoding.py
â”œâ”€â”€ integration/         # Integration tests
â”œâ”€â”€ reproducibility/     # Reproducibility tests
â”œâ”€â”€ spark/              # Spark-specific tests
â””â”€â”€ conftest.py         # Shared fixtures
```

## Running Tests

### Run All Tests
```bash
pytest tests/
```

### Run Specific Test Suite
```bash
pytest tests/unit/
pytest tests/unit/test_models.py
pytest tests/unit/test_evaluation.py
```

### Run with Coverage
```bash
pytest tests/ --cov=src/cr_score --cov-report=html --cov-report=term
```

### Run Specific Test Class
```bash
pytest tests/unit/test_models.py::TestLogisticScorecard
```

### Run Specific Test
```bash
pytest tests/unit/test_models.py::TestLogisticScorecard::test_fit
```

## Test Coverage

### New Modules (60+ tests)

#### Model Families (test_models.py)
- âœ… LogisticScorecard (12 tests)
  - Initialization, fit, predict, predict_proba
  - Sample weights, coefficients, feature importance
  - Performance metrics, export
- âœ… RandomForestScorecard (5 tests)
  - Initialization, fit, predict
  - Feature importance, tree depth statistics
- âœ… XGBoostScorecard (2 tests, optional dependency)
  - Initialization, fit and predict
- âœ… LightGBMScorecard (2 tests, optional dependency)
  - Initialization, fit and predict
- âœ… Sklearn Compatibility (2 tests)
  - clone() support
  - cross_val_score() support

#### Evaluation Module (test_evaluation.py)
- âœ… ClassificationMetrics (6 tests)
  - Accuracy, precision, recall, F1, MCC
  - Confusion matrix, optimal threshold
- âœ… StabilityMetrics (5 tests)
  - PSI calculation and interpretation
  - CSI calculation
  - Feature-level stability
- âœ… CalibrationMetrics (4 tests)
  - Brier score, log loss, ECE
  - Calibration curve
- âœ… RankingMetrics (6 tests)
  - AUC, Gini, KS statistic
  - Lift curve, gains curve
- âœ… PerformanceEvaluator (4 tests)
  - Comprehensive evaluation
  - Stability evaluation
  - Summary generation
  - Model comparison

#### Reporting Module (test_reporting.py)
- âœ… ReportExporter (6 tests)
  - JSON export
  - CSV export (multiple files)
  - Excel export (multi-sheet)
  - Markdown export
  - Comprehensive report generation
  - Serialization utilities

#### Monitoring Module (test_monitoring.py)
- âœ… PerformanceMonitor (4 tests)
  - Initialization with baselines
  - Recording predictions
  - Health check
  - Metrics summary
- âœ… DriftMonitor (3 tests)
  - Initialization
  - Drift detection
  - Drift summary
- âœ… AlertManager (4 tests)
  - Alert creation
  - Getting active alerts
  - Resolving alerts
  - Alert summary
- âœ… MetricsCollector (6 tests)
  - Counter increment
  - Gauge setting
  - Histogram recording
  - Getting metrics
  - Resetting metrics

### Coverage Goals

- **Target:** 70%+ overall coverage
- **Critical Modules:** 80%+ coverage
  - Model families: 85%
  - Evaluation: 90%
  - Reporting: 75%
  - Monitoring: 80%

## CI/CD Integration

### GitHub Actions Workflow

The test suite runs automatically on:
- Push to main branch
- Pull requests
- Scheduled daily runs

Workflow includes:
1. **Multi-Python Testing** (3.9, 3.10, 3.11)
2. **Linting** (flake8, black)
3. **Coverage Report** (pytest-cov)
4. **Optional Dependencies** (graceful skipping)

### Badges

Add to README.md:
```markdown
![Tests](https://github.com/edmunlee87/CR_Score/workflows/CI/badge.svg)
![Coverage](https://codecov.io/gh/edmunlee87/CR_Score/branch/main/graph/badge.svg)
```

## Test Fixtures

### Common Fixtures (conftest.py)

- `sample_data`: Binary classification data (1000 samples, 10 features)
- `sample_metrics`: Mock performance metrics
- `sample_model`: Mock model for testing
- `baseline_metrics`: Baseline metrics for monitoring

## Optional Dependencies

Tests handle optional dependencies gracefully:

```python
@pytest.mark.skipif(
    not pytest.importorskip("xgboost", minversion=None),
    reason="XGBoost not installed"
)
def test_xgboost_feature():
    # Test only runs if XGBoost is installed
    pass
```

## Writing New Tests

### Test Naming Convention
- File: `test_<module_name>.py`
- Class: `Test<ClassName>`
- Method: `test_<what_it_tests>`

### Example Test
```python
class TestNewFeature:
    """Tests for new feature."""
    
    def test_basic_functionality(self, sample_data):
        """Test basic functionality."""
        # Arrange
        feature = NewFeature(param=value)
        
        # Act
        result = feature.process(sample_data)
        
        # Assert
        assert result is not None
        assert len(result) > 0
```

### Using Fixtures
```python
@pytest.fixture
def custom_data():
    """Generate custom test data."""
    return pd.DataFrame({'col1': [1, 2, 3]})

def test_with_fixture(custom_data):
    """Test using custom fixture."""
    assert len(custom_data) == 3
```

## Continuous Integration

### Local Pre-commit Checks
```bash
# Run tests
pytest tests/

# Check coverage
pytest tests/ --cov=src/cr_score

# Run linting
flake8 src/
black src/ --check
```

### CI/CD Requirements
- All tests must pass
- Coverage must be >= 70%
- No linting errors
- Documentation builds successfully

## Troubleshooting

### Common Issues

1. **Import Errors**
   ```bash
   # Ensure package is installed
   pip install -e .
   ```

2. **Optional Dependency Tests Failing**
   ```bash
   # Install optional dependencies
   pip install xgboost lightgbm
   ```

3. **Coverage Not Working**
   ```bash
   # Install pytest-cov
   pip install pytest-cov
   ```

4. **Tests Run Slowly**
   ```bash
   # Run in parallel
   pytest tests/ -n auto
   ```

## Test Metrics

### Current Status
- **Total Tests:** 60+ test methods
- **Test Files:** 7 files
- **Coverage:** Target 70%+ (actual TBD after CI run)
- **Duration:** <30 seconds (unit tests only)

### Module Coverage
| Module | Tests | Status |
|--------|-------|--------|
| Models | 23 | âœ… Complete |
| Evaluation | 25 | âœ… Complete |
| Reporting | 6 | âœ… Complete |
| Monitoring | 17 | âœ… Complete |
| Feature Selection | 3 | âš ï¸  Needs update |
| Pipeline | 2 | âš ï¸  Needs update |
| WoE Encoding | 9 | âš ï¸  Needs API fix |

## Next Steps

1. âœ… Add tests for new model families
2. âœ… Add tests for evaluation module
3. âœ… Add tests for reporting module
4. âœ… Add tests for monitoring module
5. ðŸ”„ Update existing tests for current API
6. ðŸ”„ Add integration tests
7. ðŸ”„ Add reproducibility tests
8. ðŸ”„ Achieve 70%+ coverage

## Contributing

When adding new features:
1. Write tests first (TDD)
2. Ensure tests pass locally
3. Check coverage locally
4. Submit PR with tests included
5. Wait for CI/CD to pass

## Resources

- [pytest documentation](https://docs.pytest.org/)
- [pytest-cov documentation](https://pytest-cov.readthedocs.io/)
- [Testing Best Practices](https://docs.python-guide.org/writing/tests/)
