{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Default Feature Engineering\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Spark as the default engine for feature engineering\n",
    "- Automatic engine detection\n",
    "- Enhanced features with Spark (temporal, categorical, validation)\n",
    "- Pipeline with Spark DataFrames\n",
    "- Performance comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check PySpark availability\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import DataFrame as SparkDataFrame\n",
    "    PYSPARK_AVAILABLE = True\n",
    "    print(\"✓ PySpark is available\")\n",
    "except ImportError:\n",
    "    PYSPARK_AVAILABLE = False\n",
    "    print(\"⚠ PySpark not available - some examples will be skipped\")\n",
    "\n",
    "from cr_score.features import (\n",
    "    create_feature_engineer,\n",
    "    create_feature_engineer_auto,\n",
    "    FeatureEngineer,\n",
    "    FeatureEngineeringConfig,\n",
    "    FeatureRecipe,\n",
    "    AggregationType,\n",
    "    TemporalTrendFeatures,\n",
    "    CategoricalEncoder,\n",
    "    FeatureValidator,\n",
    ")\n",
    "\n",
    "from cr_score.pipeline import ScorecardPipeline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"\\n✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark as Default Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Factory Function Defaults to Spark\n",
    "\n",
    "The `create_feature_engineer()` function now defaults to Spark for large-scale processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    config = FeatureEngineeringConfig(\n",
    "        recipes=[\n",
    "            FeatureRecipe(\"max_balance_3m\", \"balance\", AggregationType.MAX, window=\"last_3_months\"),\n",
    "            FeatureRecipe(\"avg_util\", \"utilization\", AggregationType.MEAN),\n",
    "        ],\n",
    "        id_col=\"customer_id\"\n",
    "    )\n",
    "    \n",
    "    # Default: Spark engine\n",
    "    engineer = create_feature_engineer(config)\n",
    "    print(f\"Default engine: {type(engineer).__name__}\")\n",
    "    print(\"✓ Factory function defaults to Spark\")\n",
    "else:\n",
    "    print(\"⚠ PySpark not available - skipping Spark examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Explicit Engine Selection\n",
    "\n",
    "You can still explicitly choose pandas if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FeatureEngineeringConfig(\n",
    "    recipes=[\n",
    "        FeatureRecipe(\"max_balance_3m\", \"balance\", AggregationType.MAX, window=\"last_3_months\"),\n",
    "    ],\n",
    "    id_col=\"customer_id\"\n",
    ")\n",
    "\n",
    "# Explicit pandas engine\n",
    "engineer_pandas = create_feature_engineer(config, engine=\"pandas\")\n",
    "print(f\"Explicit pandas engine: {type(engineer_pandas).__name__}\")\n",
    "print(\"✓ Backward compatibility maintained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automatic Engine Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Auto-Detection Function\n",
    "\n",
    "The `create_feature_engineer_auto()` function automatically detects DataFrame type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "df_pandas = pd.DataFrame({\n",
    "    'customer_id': [1, 1, 1, 2, 2, 2],\n",
    "    'date': pd.date_range('2024-01-01', periods=6, freq='M'),\n",
    "    'balance': [1000, 1200, 1100, 2000, 2100, 2050],\n",
    "    'utilization': [0.3, 0.35, 0.32, 0.5, 0.55, 0.52],\n",
    "})\n",
    "\n",
    "print(\"Sample pandas DataFrame:\")\n",
    "print(df_pandas.head())\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    spark = SparkSession.builder.appName(\"CR_Score_Playbook\").getOrCreate()\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    print(\"\\n✓ Spark DataFrame created\")\n",
    "    \n",
    "    # Auto-detect from DataFrame type\n",
    "    config = FeatureEngineeringConfig(\n",
    "        recipes=[FeatureRecipe(\"max_balance\", \"balance\", AggregationType.MAX)],\n",
    "        id_col=\"customer_id\"\n",
    "    )\n",
    "    \n",
    "    engineer_spark = create_feature_engineer_auto(df_spark, config)\n",
    "    print(f\"Auto-detected engine for Spark DataFrame: {type(engineer_spark).__name__}\")\n",
    "    \n",
    "    engineer_pandas_auto = create_feature_engineer_auto(df_pandas, config, prefer_spark=False)\n",
    "    print(f\"Auto-detected engine for pandas DataFrame (prefer_spark=False): {type(engineer_pandas_auto).__name__}\")\n",
    "    \n",
    "    engineer_pandas_spark = create_feature_engineer_auto(df_pandas, config, prefer_spark=True)\n",
    "    print(f\"Auto-detected engine for pandas DataFrame (prefer_spark=True): {type(engineer_pandas_spark).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Unified FeatureEngineer Class\n",
    "\n",
    "The `FeatureEngineer` class provides a unified interface with automatic detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FeatureEngineeringConfig(\n",
    "    recipes=[FeatureRecipe(\"max_balance\", \"balance\", AggregationType.MAX)],\n",
    "    id_col=\"customer_id\"\n",
    ")\n",
    "\n",
    "# Unified engineer - works with both pandas and Spark\n",
    "engineer = FeatureEngineer(config)\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    # Works with Spark DataFrame\n",
    "    result_spark = engineer.fit_transform(df_spark)\n",
    "    print(f\"Detected engine: {engineer.detected_engine}\")\n",
    "    print(f\"Result type: {type(result_spark).__name__}\")\n",
    "    print(\"\\n✓ Auto-detection working with Spark\")\n",
    "\n",
    "# Works with pandas DataFrame\n",
    "result_pandas = engineer.fit_transform(df_pandas)\n",
    "print(f\"Detected engine: {engineer.detected_engine}\")\n",
    "print(f\"Result type: {type(result_pandas).__name__}\")\n",
    "print(\"\\n✓ Auto-detection working with pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Features with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Temporal Trend Features\n",
    "\n",
    "Temporal features automatically detect Spark DataFrames and use Spark implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    trend = TemporalTrendFeatures()\n",
    "    \n",
    "    # Delta feature\n",
    "    df_delta = trend.delta(df_spark, \"balance\", time_col=\"date\", group_cols=[\"customer_id\"])\n",
    "    print(\"✓ Delta feature created with Spark\")\n",
    "    print(f\"Result type: {type(df_delta).__name__}\")\n",
    "    \n",
    "    # Momentum feature\n",
    "    df_momentum = trend.momentum(df_spark, \"balance\", time_col=\"date\", group_cols=[\"customer_id\"], window=3)\n",
    "    print(\"✓ Momentum feature created with Spark\")\n",
    "    \n",
    "    # Volatility feature\n",
    "    df_volatility = trend.volatility(df_spark, \"balance\", time_col=\"date\", group_cols=[\"customer_id\"], window=3)\n",
    "    print(\"✓ Volatility feature created with Spark\")\n",
    "    \n",
    "    # Show results\n",
    "    print(\"\\nSample results:\")\n",
    "    df_delta.select(\"customer_id\", \"date\", \"balance\", \"balance_delta\").show(10)\n",
    "else:\n",
    "    print(\"⚠ PySpark not available - skipping Spark temporal features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Encoding\n",
    "\n",
    "Categorical encoding automatically uses Spark with broadcast joins for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    # Add categorical column\n",
    "    from pyspark.sql import functions as F\n",
    "    df_cat = df_spark.withColumn(\"account_type\", F.when(F.col(\"customer_id\") == 1, \"Premium\").otherwise(\"Standard\"))\n",
    "    \n",
    "    encoder = CategoricalEncoder()\n",
    "    \n",
    "    # Frequency encoding\n",
    "    df_freq = encoder.freq_encoding(df_cat, \"account_type\")\n",
    "    print(\"✓ Frequency encoding with Spark\")\n",
    "    print(f\"Result type: {type(df_freq).__name__}\")\n",
    "    \n",
    "    # Show results\n",
    "    df_freq.select(\"account_type\", \"account_type_freq\").distinct().show()\n",
    "else:\n",
    "    print(\"⚠ PySpark not available - skipping Spark categorical encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Validation\n",
    "\n",
    "Feature validation uses Spark aggregations for efficient computation on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    validator = FeatureValidator(\n",
    "        warning_thresholds={'missing_rate': 0.05},\n",
    "        hard_fail_thresholds={'missing_rate': 0.20}\n",
    "    )\n",
    "    \n",
    "    results = validator.validate_features(df_spark, feature_list=[\"balance\", \"utilization\"])\n",
    "    print(\"✓ Feature validation with Spark\")\n",
    "    \n",
    "    # Convert to DataFrame for display\n",
    "    results_df = validator.to_dataframe()\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(results_df)\n",
    "else:\n",
    "    print(\"⚠ PySpark not available - skipping Spark validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pipeline Auto-Detection\n",
    "\n",
    "The `ScorecardPipeline` now supports Spark DataFrames with automatic detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    # Create sample training data with target\n",
    "    train_data = pd.DataFrame({\n",
    "        'age': np.random.randint(18, 70, 1000),\n",
    "        'income': np.random.randint(20000, 150000, 1000),\n",
    "        'credit_score': np.random.randint(300, 850, 1000),\n",
    "        'default': np.random.binomial(1, 0.1, 1000),\n",
    "    })\n",
    "    \n",
    "    train_spark = spark.createDataFrame(train_data)\n",
    "    \n",
    "    # Pipeline with Spark DataFrame\n",
    "    pipeline = ScorecardPipeline(max_n_bins=5, prefer_spark=True)\n",
    "    \n",
    "    print(\"Fitting pipeline with Spark DataFrame...\")\n",
    "    pipeline.fit(train_spark, target_col=\"default\")\n",
    "    \n",
    "    print(f\"\\n✓ Pipeline fitted with engine: {pipeline.engine_}\")\n",
    "    \n",
    "    # Create test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'age': np.random.randint(18, 70, 100),\n",
    "        'income': np.random.randint(20000, 150000, 100),\n",
    "        'credit_score': np.random.randint(300, 850, 100),\n",
    "    })\n",
    "    \n",
    "    test_spark = spark.createDataFrame(test_data)\n",
    "    \n",
    "    # Predict with Spark DataFrame\n",
    "    scores = pipeline.predict(test_spark)\n",
    "    print(f\"\\n✓ Predictions generated: {len(scores)} scores\")\n",
    "    print(f\"Score range: {scores.min():.0f} - {scores.max():.0f}\")\n",
    "else:\n",
    "    print(\"⚠ PySpark not available - skipping Spark pipeline example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Small Dataset (Pandas Advantage)\n",
    "\n",
    "For small datasets, pandas may be faster due to overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Small dataset\n",
    "df_small = pd.DataFrame({\n",
    "    'customer_id': np.repeat(range(100), 12),\n",
    "    'date': pd.date_range('2024-01-01', periods=1200, freq='M'),\n",
    "    'balance': np.random.randn(1200) * 1000 + 5000,\n",
    "})\n",
    "\n",
    "config = FeatureEngineeringConfig(\n",
    "    recipes=[FeatureRecipe(\"max_balance\", \"balance\", AggregationType.MAX)],\n",
    "    id_col=\"customer_id\"\n",
    ")\n",
    "\n",
    "# Pandas\n",
    "start = time.time()\n",
    "engineer_pandas = create_feature_engineer(config, engine=\"pandas\")\n",
    "result_pandas = engineer_pandas.fit_transform(df_small)\n",
    "time_pandas = time.time() - start\n",
    "\n",
    "print(f\"Pandas execution time: {time_pandas:.3f}s\")\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    df_small_spark = spark.createDataFrame(df_small)\n",
    "    \n",
    "    start = time.time()\n",
    "    engineer_spark = create_feature_engineer(config, engine=\"spark\")\n",
    "    result_spark = engineer_spark.fit_transform(df_small_spark)\n",
    "    time_spark = time.time() - start\n",
    "    \n",
    "    print(f\"Spark execution time: {time_spark:.3f}s\")\n",
    "    print(f\"\\nFor small datasets, pandas is {time_spark/time_pandas:.1f}x {'faster' if time_pandas < time_spark else 'slower'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Large Dataset (Spark Advantage)\n",
    "\n",
    "For large datasets, Spark provides significant performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    # Larger dataset (10K customers, 12 months each = 120K rows)\n",
    "    print(\"Creating large dataset (120K rows)...\")\n",
    "    \n",
    "    df_large = pd.DataFrame({\n",
    "        'customer_id': np.repeat(range(10000), 12),\n",
    "        'date': pd.date_range('2024-01-01', periods=120000, freq='M'),\n",
    "        'balance': np.random.randn(120000) * 1000 + 5000,\n",
    "    })\n",
    "    \n",
    "    config = FeatureEngineeringConfig(\n",
    "        recipes=[FeatureRecipe(\"max_balance\", \"balance\", AggregationType.MAX)],\n",
    "        id_col=\"customer_id\"\n",
    "    )\n",
    "    \n",
    "    # Pandas (may be slow or OOM)\n",
    "    try:\n",
    "        start = time.time()\n",
    "        engineer_pandas = create_feature_engineer(config, engine=\"pandas\")\n",
    "        result_pandas = engineer_pandas.fit_transform(df_large)\n",
    "        time_pandas = time.time() - start\n",
    "        print(f\"Pandas execution time: {time_pandas:.3f}s\")\n",
    "    except MemoryError:\n",
    "        print(\"⚠ Pandas ran out of memory (expected for large datasets)\")\n",
    "        time_pandas = None\n",
    "    \n",
    "    # Spark\n",
    "    df_large_spark = spark.createDataFrame(df_large)\n",
    "    \n",
    "    start = time.time()\n",
    "    engineer_spark = create_feature_engineer(config, engine=\"spark\")\n",
    "    result_spark = engineer_spark.fit_transform(df_large_spark)\n",
    "    time_spark = time.time() - start\n",
    "    \n",
    "    print(f\"Spark execution time: {time_spark:.3f}s\")\n",
    "    \n",
    "    if time_pandas:\n",
    "        print(f\"\\nFor large datasets, Spark is {time_pandas/time_spark:.1f}x faster\")\n",
    "    else:\n",
    "        print(\"\\n✓ Spark handles large datasets that pandas cannot\")\n",
    "else:\n",
    "    print(\"⚠ PySpark not available - skipping large dataset comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways:\n",
    "\n",
    "1. **Spark is Default**: Factory function defaults to Spark for large-scale processing\n",
    "2. **Auto-Detection**: Unified interface automatically detects DataFrame type\n",
    "3. **Enhanced Features**: All enhanced features work seamlessly with Spark\n",
    "4. **Pipeline Support**: ScorecardPipeline supports Spark DataFrames end-to-end\n",
    "5. **Backward Compatible**: Existing pandas code continues to work\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "- **Spark (Default)**: Large datasets (>100K rows), distributed processing, production pipelines\n",
    "- **Pandas (Explicit)**: Small datasets (<10K rows), quick prototyping, single-machine development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Spark Default Feature Engineering - Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Spark is now the default engine for feature engineering\")\n",
    "print(\"✓ Auto-detection works seamlessly\")\n",
    "print(\"✓ Enhanced features support Spark\")\n",
    "print(\"✓ Pipeline supports Spark DataFrames\")\n",
    "print(\"\\nAll feature engineering processes can now be done in Spark by default!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
