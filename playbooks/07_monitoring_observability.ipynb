{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CR_Score Playbook 07: Production Monitoring & Observability\n",
    "\n",
    "**Level:** Advanced  \n",
    "**Time:** 35-40 minutes  \n",
    "**Goal:** Master production monitoring, drift detection, and observability\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Population Stability Index (PSI) for drift detection\n",
    "- Characteristic Stability Index (CSI)\n",
    "- Performance monitoring over time\n",
    "- Alert management and notification\n",
    "- Metrics collection (Prometheus-compatible)\n",
    "- SHAP explainability for model interpretability\n",
    "- Regulatory-compliant reason codes (FCRA/ECOA)\n",
    "- Interactive observability dashboards\n",
    "- Exporting comprehensive reports\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Playbook 01 or 06\n",
    "- Understanding of model monitoring concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from cr_score import ScorecardPipeline\n",
    "from cr_score.monitoring import (\n",
    "    PerformanceMonitor,\n",
    "    DriftMonitor,\n",
    "    PredictionMonitor,\n",
    "    AlertManager,\n",
    "    MetricsCollector\n",
    ")\n",
    "from cr_score.explainability import (\n",
    "    SHAPExplainer,\n",
    "    ReasonCodeGenerator,\n",
    "    FeatureImportanceAnalyzer\n",
    ")\n",
    "from cr_score.reporting import ObservabilityDashboard, ReportExporter\n",
    "\n",
    "print(\"[OK] Libraries imported!\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"Training: {len(train_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train scorecard\n",
    "pipeline = ScorecardPipeline(max_n_bins=5, pdo=20, base_score=600)\n",
    "pipeline.fit(train_df, target_col='default')\n",
    "\n",
    "# Get predictions for both train and test\n",
    "train_scores = pipeline.predict(train_df)\n",
    "train_probas = pipeline.predict_proba(train_df)\n",
    "\n",
    "test_scores = pipeline.predict(test_df)\n",
    "test_probas = pipeline.predict_proba(test_df)\n",
    "\n",
    "print(\"[OK] Model trained and predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Population Stability Index (PSI)\n",
    "\n",
    "PSI measures distribution shifts between training and production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cr_score.evaluation import StabilityMetrics\n",
    "\n",
    "# Calculate PSI between train and test scores\n",
    "psi = StabilityMetrics.calculate_psi(\n",
    "    expected=train_scores,\n",
    "    actual=test_scores,\n",
    "    bins=10\n",
    ")\n",
    "\n",
    "status = StabilityMetrics.psi_interpretation(psi)\n",
    "\n",
    "print(f\"PSI: {psi:.4f}\")\n",
    "print(f\"Status: {status.upper()}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  PSI < 0.1:  No significant change (STABLE)\")\n",
    "print(\"  0.1-0.2:    Moderate change (WARNING)\")\n",
    "print(\"  PSI > 0.2:  Significant change (CRITICAL)\")\n",
    "\n",
    "# Get detailed breakdown\n",
    "psi_breakdown = StabilityMetrics.calculate_psi_breakdown(\n",
    "    expected=train_scores,\n",
    "    actual=test_scores,\n",
    "    bins=10\n",
    ")\n",
    "\n",
    "print(\"\\nPSI Breakdown by Bin:\")\n",
    "print(psi_breakdown[['bin_label', 'expected_percent', 'actual_percent', 'psi']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature-Level Stability\n",
    "\n",
    "Check PSI for individual features to find which ones are drifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features\n",
    "numeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "feature_cols = [col for col in numeric_cols if col not in ['application_id', 'default']]\n",
    "\n",
    "# Calculate feature stability\n",
    "stability = StabilityMetrics.calculate_feature_stability(\n",
    "    expected_df=train_df,\n",
    "    actual_df=test_df,\n",
    "    features=feature_cols,\n",
    "    bins=10\n",
    ")\n",
    "\n",
    "print(\"Feature Stability Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(stability.head(15).to_string(index=False))\n",
    "\n",
    "# Count by status\n",
    "print(\"\\nSummary:\")\n",
    "print(stability['status'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Performance Monitoring\n",
    "\n",
    "Set up baseline and monitor performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "# Calculate baseline metrics on training data\n",
    "train_pred = (train_probas > 0.5).astype(int)\n",
    "baseline_metrics = {\n",
    "    'auc': roc_auc_score(train_df['default'], train_probas),\n",
    "    'precision': precision_score(train_df['default'], train_pred),\n",
    "    'recall': recall_score(train_df['default'], train_pred)\n",
    "}\n",
    "\n",
    "print(\"Baseline Metrics (Training Data):\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Initialize performance monitor\n",
    "perf_monitor = PerformanceMonitor(\n",
    "    baseline_metrics=baseline_metrics,\n",
    "    alert_thresholds={'auc': 0.05, 'precision': 0.10, 'recall': 0.10}\n",
    ")\n",
    "\n",
    "print(\"\\n[OK] Performance monitor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor test set performance\n",
    "test_pred = (test_probas > 0.5).astype(int)\n",
    "\n",
    "metrics = perf_monitor.record_predictions(\n",
    "    y_true=test_df['default'],\n",
    "    y_pred=test_pred,\n",
    "    y_proba=test_probas,\n",
    "    metadata={'dataset': 'test', 'date': '2026-01-16'}\n",
    ")\n",
    "\n",
    "print(\"Current Metrics (Test Data):\")\n",
    "for metric in ['auc', 'precision', 'recall']:\n",
    "    current = metrics[metric]\n",
    "    baseline = baseline_metrics[metric]\n",
    "    diff = current - baseline\n",
    "    print(f\"  {metric}: {current:.4f} (baseline: {baseline:.4f}, diff: {diff:+.4f})\")\n",
    "\n",
    "# Check health\n",
    "health = perf_monitor.check_health()\n",
    "print(f\"\\nHealth Status: {health['status'].upper()}\")\n",
    "\n",
    "if health['alerts']:\n",
    "    print(\"\\nALERTS:\")\n",
    "    for alert in health['alerts']:\n",
    "        print(f\"  - {alert['metric']}: {alert['degradation_pct']:.1%} degradation\")\n",
    "else:\n",
    "    print(\"No alerts - model is performing well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drift Detection Monitor\n",
    "\n",
    "Initialize drift monitor and detect distribution changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize drift monitor with training data as reference\n",
    "drift_monitor = DriftMonitor(\n",
    "    reference_data=train_df[feature_cols],\n",
    "    psi_threshold=0.1,\n",
    "    ks_threshold=0.05\n",
    ")\n",
    "\n",
    "# Detect drift in test data\n",
    "drift_report = drift_monitor.detect_drift(test_df[feature_cols])\n",
    "\n",
    "print(\"Drift Detection Report:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall Status: {drift_report['overall_status'].upper()}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Critical: {drift_report['drift_summary']['critical']} features\")\n",
    "print(f\"  Warning:  {drift_report['drift_summary']['warning']} features\")\n",
    "print(f\"  Stable:   {drift_report['drift_summary']['stable']} features\")\n",
    "\n",
    "# Show top drifted features\n",
    "print(\"\\nTop 5 Drifted Features:\")\n",
    "feature_drifts = [(feat, results['psi']) \n",
    "                  for feat, results in drift_report['feature_results'].items()]\n",
    "feature_drifts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feat, psi in feature_drifts[:5]:\n",
    "    status_emoji = 'ðŸ”´' if psi > 0.2 else 'ðŸŸ¡' if psi > 0.1 else 'ðŸŸ¢'\n",
    "    print(f\"  {feat}: PSI = {psi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: SHAP Explainability\n",
    "\n",
    "Understand what drives model predictions using SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get WoE-encoded features (what model actually uses)\n",
    "X_woe = pipeline.woe_encoder_.transform(test_df[feature_cols])\n",
    "\n",
    "# Create SHAP explainer\n",
    "shap_explainer = SHAPExplainer(\n",
    "    model=pipeline.model_,\n",
    "    model_type='linear'  # Our LogisticScorecard\n",
    ")\n",
    "\n",
    "# Fit on sample of data (for speed)\n",
    "shap_explainer.fit(X_woe, sample_size=100)\n",
    "\n",
    "# Get global feature importance\n",
    "shap_importance = shap_explainer.get_feature_importance(X_woe)\n",
    "\n",
    "print(\"SHAP Feature Importance (Global):\")\n",
    "print(\"=\"*60)\n",
    "print(shap_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n[OK] SHAP analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Reason Codes (Regulatory Compliance)\n",
    "\n",
    "Generate FCRA/ECOA-compliant adverse action reason codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reason code generator\n",
    "reason_generator = ReasonCodeGenerator(\n",
    "    model=pipeline.model_,\n",
    "    feature_names=pipeline.selected_features_\n",
    ")\n",
    "\n",
    "# Find declined applications (score < 620)\n",
    "declined_mask = test_scores < 620\n",
    "declined_apps = test_df[declined_mask].head(5)  # First 5 examples\n",
    "\n",
    "print(f\"Analyzing {len(declined_apps)} declined applications...\\n\")\n",
    "\n",
    "for idx in declined_apps.index:\n",
    "    app_id = declined_apps.loc[idx, 'application_id']\n",
    "    score = test_scores[idx]\n",
    "    \n",
    "    # Generate top 4 reasons\n",
    "    reasons = reason_generator.generate_reasons(\n",
    "        x=test_df.loc[idx, pipeline.selected_features_],\n",
    "        score=score,\n",
    "        threshold=620,\n",
    "        num_reasons=4\n",
    "    )\n",
    "    \n",
    "    print(f\"Application {app_id} (Score: {score:.0f}):\")\n",
    "    for rank, reason in enumerate(reasons, 1):\n",
    "        print(f\"  {rank}. {reason['code']}: {reason['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Metrics Collection (Prometheus-Compatible)\n",
    "\n",
    "Collect system metrics for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics collector\n",
    "metrics_collector = MetricsCollector(enable_prometheus=True)\n",
    "\n",
    "# Record various metrics\n",
    "metrics_collector.increment_counter('predictions_total', value=len(test_df))\n",
    "metrics_collector.set_gauge('model_auc', value=metrics['auc'])\n",
    "metrics_collector.set_gauge('psi_score', value=psi)\n",
    "metrics_collector.record_histogram('score_value', value=test_scores.mean())\n",
    "\n",
    "# Get all metrics\n",
    "all_metrics = metrics_collector.get_metrics()\n",
    "\n",
    "print(\"Collected Metrics:\")\n",
    "print(\"=\"*60)\n",
    "for metric_name, metric_data in all_metrics.items():\n",
    "    print(f\"{metric_name}: {metric_data['value']} ({metric_data['type']})\")\n",
    "\n",
    "print(\"\\n[OK] Metrics collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Alert Management\n",
    "\n",
    "Create and manage alerts for critical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cr_score.monitoring.alert_manager import AlertSeverity\n",
    "\n",
    "# Initialize alert manager\n",
    "alert_manager = AlertManager()\n",
    "\n",
    "# Create alerts based on monitoring results\n",
    "if health['status'] == 'critical':\n",
    "    for alert_info in health['alerts']:\n",
    "        alert = alert_manager.create_alert(\n",
    "            title=f\"Performance Degradation: {alert_info['metric']}\",\n",
    "            severity=AlertSeverity.CRITICAL,\n",
    "            details=alert_info,\n",
    "            source='performance_monitor'\n",
    "        )\n",
    "        print(f\"ALERT CREATED: {alert['title']}\")\n",
    "\n",
    "if drift_report['overall_status'] == 'critical':\n",
    "    alert = alert_manager.create_alert(\n",
    "        title='Critical Data Drift Detected',\n",
    "        severity=AlertSeverity.CRITICAL,\n",
    "        details=drift_report['drift_summary'],\n",
    "        source='drift_monitor'\n",
    "    )\n",
    "    print(f\"ALERT CREATED: {alert['title']}\")\n",
    "\n",
    "# Get all active alerts\n",
    "active_alerts = alert_manager.get_active_alerts()\n",
    "alert_summary = alert_manager.get_alert_summary()\n",
    "\n",
    "print(f\"\\nAlert Summary:\")\n",
    "print(f\"  Total: {alert_summary['total']}\")\n",
    "print(f\"  Active: {alert_summary['active']}\")\n",
    "print(f\"  Critical: {alert_summary.get('by_severity', {}).get('critical', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Observability Dashboard\n",
    "\n",
    "Generate interactive HTML dashboard with all monitoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observability dashboard\n",
    "dashboard = ObservabilityDashboard(\n",
    "    title=\"Production Scorecard Monitoring Dashboard\"\n",
    ")\n",
    "\n",
    "# Add performance section\n",
    "metrics_df = perf_monitor.get_metrics_summary()\n",
    "dashboard.add_performance_section(metrics_df, health)\n",
    "\n",
    "# Add drift section\n",
    "dashboard.add_drift_section(drift_report)\n",
    "\n",
    "# Add prediction section\n",
    "pred_summary = {\n",
    "    'mean_score': test_scores.mean(),\n",
    "    'std_score': test_scores.std(),\n",
    "    'min_score': test_scores.min(),\n",
    "    'max_score': test_scores.max(),\n",
    "    'mean_proba': test_probas.mean(),\n",
    "}\n",
    "dashboard.add_prediction_section(pred_summary)\n",
    "\n",
    "# Add metrics section\n",
    "dashboard.add_metrics_section(all_metrics)\n",
    "\n",
    "# Add alerts section\n",
    "dashboard.add_alerts_section(active_alerts, alert_summary)\n",
    "\n",
    "# Export dashboard\n",
    "dashboard.export('reports/observability_dashboard.html')\n",
    "\n",
    "print(\"[OK] Observability dashboard generated!\")\n",
    "print(\"Open: reports/observability_dashboard.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Export Comprehensive Reports\n",
    "\n",
    "Export all analysis in multiple formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "exporter = ReportExporter()\n",
    "\n",
    "# Get full metrics\n",
    "full_metrics = pipeline.model_.get_performance_metrics(\n",
    "    test_df['default'],\n",
    "    test_probas,\n",
    "    include_stability=True,\n",
    "    y_train_proba=train_probas\n",
    ")\n",
    "\n",
    "# Export to all formats\n",
    "files = exporter.export_comprehensive_report(\n",
    "    model=pipeline.model_,\n",
    "    metrics=full_metrics,\n",
    "    X_test=pipeline.woe_encoder_.transform(test_df[feature_cols]),\n",
    "    y_test=test_df['default'],\n",
    "    output_dir='reports/comprehensive/',\n",
    "    formats=['json', 'csv', 'excel', 'markdown'],\n",
    "    include_curves=True\n",
    ")\n",
    "\n",
    "print(\"[OK] Comprehensive reports exported!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for fmt, paths in files.items():\n",
    "    print(f\"  {fmt}: {len(paths)} file(s) in reports/comprehensive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "1. âœ… **PSI Calculation** - Detect population shifts\n",
    "2. âœ… **Feature Stability** - Monitor individual features for drift\n",
    "3. âœ… **Performance Monitoring** - Track metrics over time with baselines\n",
    "4. âœ… **Drift Detection** - Automated monitoring with PSI/KS tests\n",
    "5. âœ… **SHAP Explainability** - Understand model decisions\n",
    "6. âœ… **Reason Codes** - FCRA/ECOA-compliant adverse action notices\n",
    "7. âœ… **Metrics Collection** - Prometheus-compatible system metrics\n",
    "8. âœ… **Alert Management** - Multi-severity alerting system\n",
    "9. âœ… **Observability Dashboard** - Interactive monitoring interface\n",
    "10. âœ… **Comprehensive Reports** - Multi-format exports\n",
    "\n",
    "### Production Checklist:\n",
    "\n",
    "- âœ… Set baseline metrics from validation data\n",
    "- âœ… Configure alert thresholds (PSI > 0.1, performance drop > 5%)\n",
    "- âœ… Monitor both feature-level and score-level drift\n",
    "- âœ… Generate reason codes for all declined applications\n",
    "- âœ… Set up automated dashboard generation (daily/weekly)\n",
    "- âœ… Configure notification channels (email, Slack, PagerDuty)\n",
    "- âœ… Document compliance procedures\n",
    "- âœ… Establish model retraining criteria\n",
    "\n",
    "### Key Metrics to Monitor:\n",
    "\n",
    "**Performance:**\n",
    "- AUC (should stay within 5% of baseline)\n",
    "- Gini coefficient\n",
    "- KS statistic\n",
    "- Brier score (calibration)\n",
    "\n",
    "**Stability:**\n",
    "- PSI < 0.1: Stable\n",
    "- PSI 0.1-0.2: Warning\n",
    "- PSI > 0.2: Critical (investigate/retrain)\n",
    "\n",
    "**Operational:**\n",
    "- Prediction latency\n",
    "- Throughput (predictions/second)\n",
    "- Error rates\n",
    "- System resource usage\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Set up automated monitoring pipeline\n",
    "2. Configure notification channels\n",
    "3. Establish model retraining schedule\n",
    "4. Document compliance procedures\n",
    "5. Create runbooks for alert responses\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- `reports/observability_dashboard.html` - Interactive monitoring dashboard\n",
    "- `reports/comprehensive/` - Full reports (JSON, CSV, Excel, Markdown)\n",
    "- Metrics, alerts, and monitoring data\n",
    "\n",
    "### Production Tips:\n",
    "\n",
    "1. **Daily Monitoring**: Run drift detection daily\n",
    "2. **Weekly Reviews**: Review performance metrics weekly\n",
    "3. **Monthly Deep Dive**: Full model health check monthly\n",
    "4. **Retrain Triggers**: PSI > 0.2 OR performance drop > 10%\n",
    "5. **Documentation**: Keep audit trail of all monitoring activities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}