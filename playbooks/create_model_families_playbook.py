"""
Generate comprehensive model families comparison playbook.
"""

import json

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CR_Score Playbook 06: Model Families Comparison\n",
                "\n",
                "**Level:** Advanced  \n",
                "**Time:** 30-35 minutes  \n",
                "**Goal:** Master all 4 model families and choose the best one\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "- Using Logistic Regression for interpretability\n",
                "- Using Random Forest for non-linear patterns  \n",
                "- Using XGBoost for high performance\n",
                "- Using LightGBM for speed\n",
                "- Comparing models with 40+ metrics (PSI, CSI, Gini, KS, etc.)\n",
                "- Feature importance comparison\n",
                "- Exporting comprehensive reports\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- Completed Playbook 01\n",
                "- XGBoost and LightGBM are optional (will work without them)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Step 1: Setup and Import All Model Families"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root / 'src'))\n",
                "\n",
                "# Import all model families\n",
                "from cr_score.model import (\n",
                "    LogisticScorecard,\n",
                "    RandomForestScorecard,\n",
                "    XGBoostScorecard,\n",
                "    LightGBMScorecard\n",
                ")\n",
                "from cr_score.encoding import WoEEncoder\n",
                "from cr_score.evaluation import PerformanceEvaluator\n",
                "from cr_score.reporting import ReportExporter\n",
                "\n",
                "print(\"âœ… Core libraries imported!\")\n",
                "\n",
                "# Check which models are available\n",
                "models_available = {\n",
                "    'Logistic': True,\n",
                "    'RandomForest': True,\n",
                "    'XGBoost': False,\n",
                "    'LightGBM': False\n",
                "}\n",
                "\n",
                "try:\n",
                "    import xgboost\n",
                "    models_available['XGBoost'] = True\n",
                "    print(\"âœ… XGBoost available\")\n",
                "except ImportError:\n",
                "    print(\"âš ï¸  XGBoost not installed (optional)\")\n",
                "\n",
                "try:\n",
                "    import lightgbm\n",
                "    models_available['LightGBM'] = True\n",
                "    print(\"âœ… LightGBM available\")\n",
                "except ImportError:\n",
                "    print(\"âš ï¸  LightGBM not installed (optional)\")\n",
                "\n",
                "print(f\"\\nğŸ“Š Will compare {sum(models_available.values())} model families\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Step 2: Load and Prepare Data"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "train_df = pd.read_csv('data/train.csv')\n",
                "test_df = pd.read_csv('data/test.csv')\n",
                "\n",
                "print(f\"Training: {len(train_df)} samples, Default rate: {train_df['default'].mean():.2%}\")\n",
                "print(f\"Test: {len(test_df)} samples, Default rate: {test_df['default'].mean():.2%}\")\n",
                "\n",
                "# Select numeric features\n",
                "numeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "feature_cols = [col for col in numeric_cols if col not in ['application_id', 'default']]\n",
                "\n",
                "print(f\"\\nUsing {len(feature_cols)} numeric features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: WoE Encoding\n",
                "\n",
                "All models work better with WoE-encoded features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create WoE encoder\n",
                "woe_encoder = WoEEncoder()\n",
                "\n",
                "# Fit and transform\n",
                "X_train_woe = woe_encoder.fit_transform(train_df[feature_cols], train_df['default'])\n",
                "X_test_woe = woe_encoder.transform(test_df[feature_cols])\n",
                "y_train = train_df['default']\n",
                "y_test = test_df['default']\n",
                "\n",
                "print(f\"âœ… WoE encoding complete\")\n",
                "print(f\"Training shape: {X_train_woe.shape}\")\n",
                "print(f\"Test shape: {X_test_woe.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Initialize All Models\n",
                "\n",
                "Let's create instances of all available model families with sensible defaults."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {}\n",
                "\n",
                "# 1. Logistic Regression (always available)\n",
                "models['Logistic'] = LogisticScorecard(random_state=42)\n",
                "print(\"âœ… Logistic Regression: Best for interpretability\")\n",
                "\n",
                "# 2. Random Forest (always available)\n",
                "models['RandomForest'] = RandomForestScorecard(\n",
                "    n_estimators=100,\n",
                "    max_depth=5,\n",
                "    random_state=42\n",
                ")\n",
                "print(\"âœ… Random Forest: Best for non-linear patterns\")\n",
                "\n",
                "# 3. XGBoost (if available)\n",
                "if models_available['XGBoost']:\n",
                "    models['XGBoost'] = XGBoostScorecard(\n",
                "        n_estimators=100,\n",
                "        max_depth=5,\n",
                "        learning_rate=0.1,\n",
                "        random_state=42\n",
                "    )\n",
                "    print(\"âœ… XGBoost: Best for high performance\")\n",
                "\n",
                "# 4. LightGBM (if available)\n",
                "if models_available['LightGBM']:\n",
                "    models['LightGBM'] = LightGBMScorecard(\n",
                "        n_estimators=100,\n",
                "        num_leaves=31,\n",
                "        learning_rate=0.1,\n",
                "        random_state=42\n",
                "    )\n",
                "    print(\"âœ… LightGBM: Best for speed and large datasets\")\n",
                "\n",
                "print(f\"\\nğŸ“Š Total models to compare: {len(models)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Train All Models\n",
                "\n",
                "Train each model and measure training time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "trained_models = {}\n",
                "predictions = {}\n",
                "training_times = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\nTraining {name}...\")\n",
                "    \n",
                "    # Train and time it\n",
                "    start_time = time.time()\n",
                "    model.fit(X_train_woe, y_train)\n",
                "    train_time = time.time() - start_time\n",
                "    \n",
                "    # Predict\n",
                "    y_pred_proba = model.predict_proba(X_test_woe)[:, 1]\n",
                "    \n",
                "    # Store\n",
                "    trained_models[name] = model\n",
                "    predictions[name] = y_pred_proba\n",
                "    training_times[name] = train_time\n",
                "    \n",
                "    print(f\"  âœ… Trained in {train_time:.2f}s\")\n",
                "\n",
                "print(f\"\\nâœ… All {len(trained_models)} models trained!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Comprehensive Metrics Evaluation\n",
                "\n",
                "Evaluate all models with 40+ metrics including PSI, CSI, Gini, KS, and more."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate all models\n",
                "all_metrics = {}\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"MODEL EVALUATION RESULTS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for name, model in trained_models.items():\n",
                "    print(f\"\\n{name}:\")\n",
                "    \n",
                "    # Get comprehensive metrics\n",
                "    metrics = model.get_performance_metrics(\n",
                "        y_test,\n",
                "        predictions[name],\n",
                "        threshold=0.5\n",
                "    )\n",
                "    \n",
                "    all_metrics[name] = metrics\n",
                "    \n",
                "    # Print key metrics\n",
                "    print(f\"  AUC:       {metrics['ranking']['auc']:.4f} {metrics['interpretations'].get('auc', '')}\")\n",
                "    print(f\"  Gini:      {metrics['ranking']['gini']:.4f} {metrics['interpretations'].get('gini', '')}\")\n",
                "    print(f\"  KS:        {metrics['ranking']['ks_statistic']:.4f} {metrics['interpretations'].get('ks', '')}\")\n",
                "    print(f\"  Brier:     {metrics['calibration']['brier_score']:.4f}\")\n",
                "    print(f\"  MCC:       {metrics['classification']['mcc']:.4f}\")\n",
                "    print(f\"  Train Time: {training_times[name]:.2f}s\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Side-by-Side Comparison\n",
                "\n",
                "Create a comparison table to see all metrics at once."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison DataFrame\n",
                "evaluator = PerformanceEvaluator()\n",
                "comparison_df = evaluator.compare_models(all_metrics)\n",
                "\n",
                "# Add training time\n",
                "comparison_df['train_time'] = comparison_df['model'].map(training_times)\n",
                "\n",
                "print(\"\\nğŸ“Š MODEL COMPARISON TABLE\")\n",
                "print(\"=\" * 80)\n",
                "print(comparison_df.to_string(index=False))\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Find best models\n",
                "best_auc_idx = comparison_df['auc'].idxmax()\n",
                "best_model_name = comparison_df.loc[best_auc_idx, 'model']\n",
                "best_auc = comparison_df.loc[best_auc_idx, 'auc']\n",
                "\n",
                "fastest_idx = comparison_df['train_time'].idxmin()\n",
                "fastest_name = comparison_df.loc[fastest_idx, 'model']\n",
                "\n",
                "print(f\"\\nğŸ† Best Performance (AUC): {best_model_name} ({best_auc:.4f})\")\n",
                "print(f\"âš¡ Fastest Training: {fastest_name} ({comparison_df.loc[fastest_idx, 'train_time']:.2f}s)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Feature Importance Comparison\n",
                "\n",
                "See which features are most important according to each model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "n_models = len(trained_models)\n",
                "fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
                "\n",
                "if n_models == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "for idx, (name, model) in enumerate(trained_models.items()):\n",
                "    importance = model.get_feature_importance().head(10)\n",
                "    \n",
                "    ax = axes[idx]\n",
                "    \n",
                "    # Get column names (they vary by model type)\n",
                "    feature_col = 'feature' if 'feature' in importance.columns else importance.columns[0]\n",
                "    importance_col = importance.columns[1]\n",
                "    \n",
                "    ax.barh(importance[feature_col], importance[importance_col])\n",
                "    ax.set_xlabel('Importance')\n",
                "    ax.set_title(f'{name}\\nTop 10 Features')\n",
                "    ax.invert_yaxis()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nâœ… Feature importance plotted!\")\n",
                "print(\"\\nNote: Different models measure importance differently:\")\n",
                "print(\"  - Logistic: Coefficient magnitude\")\n",
                "print(\"  - RandomForest: Gini importance (mean decrease in impurity)\")\n",
                "print(\"  - XGBoost: Weight (number of times feature is used)\")\n",
                "print(\"  - LightGBM: Split importance\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: ROC Curve Comparison\n",
                "\n",
                "Visualize model discrimination power."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import roc_curve, auc\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "for name in trained_models.keys():\n",
                "    fpr, tpr, _ = roc_curve(y_test, predictions[name])\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    \n",
                "    plt.plot(fpr, tpr, lw=2, \n",
                "             label=f'{name} (AUC = {roc_auc:.3f})')\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curves - Model Comparison')\n",
                "plt.legend(loc=\"lower right\")\n",
                "plt.grid(alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… ROC curves plotted!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Export Comprehensive Reports\n",
                "\n",
                "Export results in multiple formats (JSON, CSV, Excel, Markdown)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create exporter\n",
                "exporter = ReportExporter()\n",
                "\n",
                "print(\"Exporting reports for all models...\\n\")\n",
                "\n",
                "for name, model in trained_models.items():\n",
                "    clean_name = name.lower().replace(' ', '_')\n",
                "    output_dir = Path(f'reports/{clean_name}')\n",
                "    \n",
                "    print(f\"ğŸ“ Exporting {name}...\")\n",
                "    \n",
                "    # Export to all formats\n",
                "    files = exporter.export_comprehensive_report(\n",
                "        model=model,\n",
                "        metrics=all_metrics[name],\n",
                "        X_test=X_test_woe,\n",
                "        y_test=y_test,\n",
                "        output_dir=output_dir,\n",
                "        formats=['json', 'csv', 'excel', 'markdown'],\n",
                "        include_curves=True\n",
                "    )\n",
                "    \n",
                "    print(f\"  âœ… Saved to {output_dir}/\")\n",
                "    for fmt, paths in files.items():\n",
                "        print(f\"     - {fmt}: {len(paths)} file(s)\")\n",
                "    print()\n",
                "\n",
                "print(\"\\nâœ… All reports exported!\")\n",
                "print(\"\\nGenerated files:\")\n",
                "print(\"  - metrics.json (machine-readable)\")\n",
                "print(\"  - *.csv (separate files per metric category)\")\n",
                "print(\"  - scorecard_report.xlsx (comprehensive Excel)\")\n",
                "print(\"  - scorecard_report.md (human-readable Markdown)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Model Selection Guide\n",
                "\n",
                "Which model should you choose?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"MODEL SELECTION GUIDE\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\\n1ï¸âƒ£  LOGISTIC REGRESSION\")\n",
                "print(\"   ğŸ“ Best for: Interpretability, Regulatory Compliance\")\n",
                "print(\"   âœ… Pros: Clear coefficients, Easy to explain, Fast\")\n",
                "print(\"   âŒ Cons: Assumes linear relationships\")\n",
                "print(\"   ğŸ¯ Use when: You need to explain decisions to regulators/business\")\n",
                "\n",
                "print(\"\\n2ï¸âƒ£  RANDOM FOREST\")\n",
                "print(\"   ğŸŒ² Best for: Non-linear Patterns, Feature Interactions\")\n",
                "print(\"   âœ… Pros: Robust, Handles outliers well, No scaling needed\")\n",
                "print(\"   âŒ Cons: Less interpretable, Can overfit, Slower prediction\")\n",
                "print(\"   ğŸ¯ Use when: You need to capture complex patterns\")\n",
                "\n",
                "print(\"\\n3ï¸âƒ£  XGBOOST\")\n",
                "print(\"   ğŸš€ Best for: Maximum Performance, Competitions\")\n",
                "print(\"   âœ… Pros: Excellent performance, Built-in regularization, Handles missing values\")\n",
                "print(\"   âŒ Cons: More hyperparameters, Longer training, Less interpretable\")\n",
                "print(\"   ğŸ¯ Use when: Performance is top priority\")\n",
                "\n",
                "print(\"\\n4ï¸âƒ£  LIGHTGBM\")\n",
                "print(\"   âš¡ Best for: Large Datasets, Speed\")\n",
                "print(\"   âœ… Pros: Very fast, Low memory, Handles categorical features natively\")\n",
                "print(\"   âŒ Cons: Can overfit on small data, Sensitive to parameters\")\n",
                "print(\"   ğŸ¯ Use when: You have large datasets (>10K samples)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"YOUR RESULTS\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nğŸ† Best Model (AUC): {best_model_name} ({best_auc:.4f})\")\n",
                "print(f\"âš¡ Fastest Model: {fastest_name}\")\n",
                "\n",
                "print(\"\\nğŸ’¡ RECOMMENDATION:\")\n",
                "if best_model_name == 'Logistic':\n",
                "    print(\"   âœ… Use Logistic Regression - Great balance of performance and interpretability!\")\n",
                "else:\n",
                "    print(f\"   ğŸ¯ {best_model_name} has best performance ({best_auc:.4f} AUC)\")\n",
                "    logistic_auc = comparison_df[comparison_df['model'] == 'Logistic']['auc'].values[0]\n",
                "    improvement = ((best_auc - logistic_auc) / logistic_auc) * 100\n",
                "    print(f\"   ğŸ“Š That's {improvement:.1f}% better than Logistic Regression\")\n",
                "    \n",
                "    if improvement < 2:\n",
                "        print(\"   ğŸ’¼ Consider Logistic for interpretability (performance difference is small)\")\n",
                "    else:\n",
                "        print(f\"   âš–ï¸  Trade-off: {improvement:.1f}% performance gain vs. interpretability loss\")\n",
                "        print(\"   ğŸ’¼ Use Logistic if you need to explain decisions to stakeholders\")\n",
                "        print(f\"   ğŸš€ Use {best_model_name} if performance is priority\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### What You Learned:\n",
                "\n",
                "1. âœ… **Trained 4 different model families** (Logistic, RandomForest, XGBoost, LightGBM)\n",
                "2. âœ… **Compared 40+ metrics** including AUC, Gini, KS, PSI, CSI, MCC\n",
                "3. âœ… **Analyzed feature importance** differences across models\n",
                "4. âœ… **Exported comprehensive reports** in JSON, CSV, Excel, Markdown\n",
                "5. âœ… **Learned when to use each model** based on your requirements\n",
                "\n",
                "### Key Takeaways:\n",
                "\n",
                "- **All models use the same unified API** - easy to switch between them\n",
                "- **All models work with feature selection** from Playbook 02\n",
                "- **All models support sample weights** for compressed data\n",
                "- **Choose based on your priority**: Interpretability vs. Performance vs. Speed\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. Try different hyperparameters for each model\n",
                "2. Use feature selection to improve performance\n",
                "3. Check Playbook 07 for advanced reporting features\n",
                "4. Compare calibration curves across models\n",
                "5. Monitor PSI for model stability over time\n",
                "\n",
                "### Files Generated:\n",
                "\n",
                "Check the `reports/` directory for each model:\n",
                "- `reports/logistic/` - Logistic Regression reports\n",
                "- `reports/randomforest/` - Random Forest reports  \n",
                "- `reports/xgboost/` - XGBoost reports (if installed)\n",
                "- `reports/lightgbm/` - LightGBM reports (if installed)\n",
                "\n",
                "Each contains:\n",
                "- `metrics.json` - All metrics in JSON format\n",
                "- `*.csv` - Individual metric categories\n",
                "- `scorecard_report.xlsx` - Comprehensive Excel report\n",
                "- `scorecard_report.md` - Markdown documentation"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Write notebook
with open('06_model_families.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print("[OK] Model families playbook created: 06_model_families.ipynb")
